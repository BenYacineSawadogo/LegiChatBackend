from flask import Flask, render_template, request, Response, session, jsonify
from flask_cors import CORS
import time
import re
import pickle
import unicodedata
import numpy as np
import pandas as pd
import os
import cv2
import pytesseract
from PIL import Image
from PyPDF2 import PdfReader
from pdf2image import convert_from_path
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
from mistralai import Mistral
from datetime import datetime
from collections import defaultdict


# ==========================================================
# üöÄ Configuration Flask
# ==========================================================
app = Flask(__name__)
app.secret_key = "resume_secret_key"

# Configuration CORS pour le frontend Angular
CORS(app, origins=["http://localhost:4200"], supports_credentials=True)

# Stockage en m√©moire des conversations (remplacer par DB en production)
conversations_history = defaultdict(list)


# ==========================================================
# ‚öôÔ∏è Configuration Tesseract OCR (Windows)
# ==========================================================
pytesseract.pytesseract.tesseract_cmd = (
    r"C:\\Users\\DELL 7540\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe"
)
os.environ["TESSDATA_PREFIX"] = (
    r"C:\\Users\\DELL 7540\\AppData\\Local\\Programs\\Tesseract-OCR\\tessdata"
)


# ==========================================================
# üì¶ Chargement des mod√®les et donn√©es
# ==========================================================
encoder_model = SentenceTransformer(
    "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)
client = Mistral(api_key="uhSmZH1rHb7TPZdmoSnjVRGMrDPtJDe6")
mistral_model = "mistral-small-latest"

with open("faiss_index/metadatas.pkl", "rb") as f:
    metadatas = pickle.load(f)

embeddings = np.load("faiss_index/embeddings.npy")
df = pd.read_csv("faiss_index/fichier.csv")
textes = df["texte"].tolist()


# ==========================================================
# üí¨ Patterns conversationnels
# ==========================================================
CONVERSATIONAL_PATTERNS = [
    r'^(bonjour|salut|bonsoir|hello|hi|hey)',
    r'^(merci|thanks|au revoir|bye|adieu)',
    r'^(comment √ßa va|√ßa va|comment vas-tu)',
    r'^(ok|d\'accord|compris|entendu)',
    r'^(qui es-tu|tu es qui|qui √™tes-vous)',
    r'^(quel est ton nom|comment tu t\'appelles)',
]

CONVERSATIONAL_RESPONSES = {
    "greeting": [
        "Bonjour ! Je suis votre assistant juridique sp√©cialis√© en droit burkinab√®. Comment puis-je vous aider aujourd'hui ?",
        "Salut ! Je suis l√† pour r√©pondre √† vos questions juridiques concernant le Burkina Faso. Que puis-je faire pour vous ?"
    ],
    "thanks": [
        "Je vous en prie ! N'h√©sitez pas si vous avez d'autres questions juridiques.",
        "Avec plaisir ! Je reste √† votre disposition pour toute autre question."
    ],
    "goodbye": [
        "Au revoir ! √Ä bient√¥t pour vos prochaines questions juridiques.",
        "√Ä bient√¥t ! N'h√©sitez pas √† revenir si vous avez besoin d'aide."
    ],
    "identity": [
        "Je suis un assistant juridique sp√©cialis√© dans le droit du Burkina Faso. Je peux vous aider √† comprendre les lois, d√©crets et arr√™t√©s burkinab√®.",
        "Je suis votre assistant pour les questions juridiques relatives au Burkina Faso, bas√© sur les documents officiels du pays."
    ],
    "default": [
        "Je suis l√† pour vous aider. Posez-moi vos questions juridiques concernant le Burkina Faso.",
        "Comment puis-je vous assister dans vos recherches juridiques ?"
    ]
}


# ==========================================================
# üõ†Ô∏è Fonctions utilitaires
# ==========================================================
def preprocess_image(pil_img):
    img = np.array(pil_img.convert("L"))
    img = cv2.GaussianBlur(img, (5, 5), 0)
    _, img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    return Image.fromarray(img)


def extract_text_ocr(pdf_path):
    images = convert_from_path(pdf_path, dpi=300)
    full_text = ""
    for img in images:
        preproc = preprocess_image(img)
        text = pytesseract.image_to_string(preproc, lang="fra", config="--psm 6")
        full_text += "\n" + text
    return full_text


def extract_text_from_pdf(path):
    try:
        reader = PdfReader(path)
        text = "\n".join(page.extract_text() or "" for page in reader.pages)
        if len(text.strip()) < 100:  # si texte trop court ‚Üí fallback OCR
            return extract_text_ocr(path)
        return text
    except Exception:
        return extract_text_ocr(path)


def detecte_type_question(question):
    motifs = (
        r"\b(cherche|cherches|chercher|recherche|recherches|donne|donner|donne[-\s]?moi|"
        r"fournis|fournir|trouve|trouver|obtiens|obtenir|je veux|j'aimerais voir|"
        r"montre[-\s]?moi|acc√®de|acc√©der √†|affiche|afficher)\b"
    )
    return "recherche" if re.search(motifs, question.lower()) else "demande"


def extraire_reference_loi_decret(question):
    question = question.lower()
    pattern = r"(loi|d√©cret)[\s\-n¬∞]*([n]?\s?\d{1,4}(?:[\-/]\d{1,4})?[\-\_/]?\d{1,4}?)"
    match = re.search(pattern, question)
    if match:
        type_texte = match.group(1)
        numero = match.group(2).replace(" ", "")
        return type_texte.capitalize(), numero
    return None, None


def enlever_accents(texte):
    return "".join(
        c for c in unicodedata.normalize("NFD", texte) if unicodedata.category(c) != "Mn"
    )


def normaliser_numero(numero):
    parties = re.split(r"[-/]", numero)
    if len(parties) == 2:
        a, b = str(int(parties[0])), str(int(parties[1]))
        return [f"{a}-{b}", f"{b}-{a}"]
    return [numero]


def rechercher_dans_metadatas(type_texte, numero, metadatas):
    if type_texte and numero:
        variantes = normaliser_numero(numero)
        prefix = enlever_accents(type_texte.upper())
        for metadata in metadatas:
            loi_metadata = enlever_accents(str(metadata["loi"]).upper())
            for variante in variantes:
                terme = f"{prefix}_{variante}"
                if terme in loi_metadata:
                    return metadata["lien_pdf"]
    return None


def is_conversational_message(message):
    """
    D√©tecte si un message est conversationnel (salutations, remerciements, etc.)
    plut√¥t qu'une vraie question juridique.

    Args:
        message: Le message utilisateur

    Returns:
        tuple: (bool is_conversational, str conversation_type)
    """
    message_lower = message.lower().strip()

    # Messages tr√®s courts (< 15 caract√®res) sont probablement conversationnels
    if len(message_lower) < 15:
        # D√©tecter le type sp√©cifique
        if re.match(r'^(bonjour|salut|bonsoir|hello|hi|hey)', message_lower):
            return True, "greeting"
        elif re.match(r'^(merci|thanks)', message_lower):
            return True, "thanks"
        elif re.match(r'^(au revoir|bye|adieu)', message_lower):
            return True, "goodbye"
        elif re.match(r'^(ok|d\'accord|compris|entendu)', message_lower):
            return True, "default"

    # V√©rifier les patterns sp√©cifiques
    for pattern in CONVERSATIONAL_PATTERNS:
        if re.match(pattern, message_lower):
            # D√©terminer le type
            if any(word in message_lower for word in ["bonjour", "salut", "bonsoir", "hello", "hi", "hey"]):
                return True, "greeting"
            elif any(word in message_lower for word in ["merci", "thanks"]):
                return True, "thanks"
            elif any(word in message_lower for word in ["au revoir", "bye", "adieu"]):
                return True, "goodbye"
            elif any(word in message_lower for word in ["qui es-tu", "tu es qui", "qui √™tes-vous", "ton nom", "t'appelles"]):
                return True, "identity"
            elif any(word in message_lower for word in ["comment √ßa va", "√ßa va"]):
                return True, "greeting"
            else:
                return True, "default"

    return False, None


def generate_conversational_response(conversation_type):
    """
    G√©n√®re une r√©ponse conversationnelle appropri√©e.

    Args:
        conversation_type: Type de conversation (greeting, thanks, goodbye, identity, default)

    Returns:
        str: R√©ponse conversationnelle
    """
    import random

    responses = CONVERSATIONAL_RESPONSES.get(conversation_type, CONVERSATIONAL_RESPONSES["default"])
    return random.choice(responses)


def validate_response(response, response_type, sources):
    """
    Valide et nettoie une r√©ponse avant de l'envoyer.

    Args:
        response: Le texte de la r√©ponse
        response_type: Type de r√©ponse
        sources: Liste des sources

    Returns:
        tuple: (cleaned_response, cleaned_sources)
    """
    # Nettoyer la r√©ponse
    cleaned_response = response.strip()

    # Remplacer les tableaux vides par None
    cleaned_sources = None if not sources or len(sources) == 0 else sources

    # Si on a des sources, les nettoyer
    if cleaned_sources:
        for source in cleaned_sources:
            # Convertir les scores de pertinence en pourcentages
            if "relevance" in source and source["relevance"] is not None:
                # Convertir 0.95 en 95
                source["relevance"] = round(source["relevance"] * 100, 1)

    return cleaned_response, cleaned_sources


def encodeur(question):
    return encoder_model.encode(question)


def generate_mistral_stream(prompt):
    response = client.chat.complete(
        model=mistral_model, messages=[{"role": "user", "content": prompt}]
    )
    full_text = response.choices[0].message.content

    # Nettoyage du texte
    cleaned_text = re.sub(r"#+\s*", "", full_text)

    # Streaming par paquets de 50 caract√®res
    for i in range(0, len(cleaned_text), 50):
        yield cleaned_text[i : i + 50]
        time.sleep(0.1)


def generate_mistral_complete(messages):
    """
    G√©n√®re une r√©ponse compl√®te (non-streaming) √† partir d'un historique de messages.

    Args:
        messages: Liste de messages au format [{"role": "user|assistant", "content": "..."}]

    Returns:
        str: R√©ponse compl√®te de Mistral
    """
    response = client.chat.complete(
        model=mistral_model,
        messages=messages
    )
    full_text = response.choices[0].message.content

    # Nettoyage du texte
    cleaned_text = re.sub(r"#+\s*", "", full_text)
    return cleaned_text


def generate_message_id():
    """G√©n√®re un ID unique pour un message."""
    import random
    timestamp = int(datetime.now().timestamp() * 1000)
    random_part = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz0123456789', k=9))
    return f"msg-{timestamp}-{random_part}"


def process_question_with_context(conversation_id, new_message):
    """
    Traite une question en utilisant le contexte de la conversation.

    Args:
        conversation_id: ID de la conversation
        new_message: Nouveau message utilisateur

    Returns:
        tuple: (r√©ponse de l'assistant, type de r√©ponse, sources utilis√©es)
    """
    # üöÄ OPTIMISATION 1 : V√©rifier d'abord si c'est un message conversationnel
    # (√©vite les recherches FAISS inutiles pour les "Bonjour", "Merci", etc.)
    is_conversational, conv_type = is_conversational_message(new_message)
    if is_conversational:
        response = generate_conversational_response(conv_type)
        return response, "conversational", None

    # R√©cup√©rer l'historique de la conversation
    history = conversations_history[conversation_id]

    # D√©tecter le type de question
    type_question = detecte_type_question(new_message)

    # === Cas recherche de document ===
    if type_question == "recherche":
        type_texte, numero = extraire_reference_loi_decret(new_message)
        lien_pdf = rechercher_dans_metadatas(type_texte, numero, metadatas)

        if lien_pdf:
            response = (
                f"üìÑ Voici le document demand√© : "
                f'<a href="{lien_pdf}" target="_blank">cliquer ici</a><br>'
                f"Souhaitez-vous un r√©sum√© ? (oui/non)"
            )
            # Stocker la r√©f√©rence dans l'historique pour usage ult√©rieur
            conversations_history[conversation_id].append({
                "type": "reference",
                "lien": lien_pdf,
                "type_texte": type_texte,
                "numero": numero
            })
            sources = [{"type": type_texte, "numero": numero, "lien": lien_pdf}]
            return response, "document_link", sources
        else:
            return "‚ùå R√©f√©rence non trouv√©e dans les m√©tadonn√©es.", "not_found", None

    # === Cas r√©sum√© (si l'utilisateur r√©pond oui apr√®s une recherche de doc) ===
    if new_message.lower() in ["oui", "oui merci", "r√©sume", "r√©sume-moi", "je veux un r√©sum√©"]:
        # Chercher la derni√®re r√©f√©rence dans l'historique
        last_reference = None
        for item in reversed(history):
            if isinstance(item, dict) and item.get("type") == "reference":
                last_reference = item
                break

        if last_reference:
            nom_fichier = last_reference["lien"].split("/")[-1]
            chemin_pdf = f"./static/pdfs/{nom_fichier}"

            try:
                texte_complet = extract_text_from_pdf(chemin_pdf)
                prompt_messages = [
                    {
                        "role": "user",
                        "content": f"Voici le contenu d'un texte juridique du Burkina Faso :\n\n{texte_complet}\n\nFais un r√©sum√© clair et structur√© de ce document."
                    }
                ]
                summary = generate_mistral_complete(prompt_messages)
                sources = [{
                    "type": last_reference.get("type_texte"),
                    "numero": last_reference.get("numero"),
                    "lien": last_reference["lien"]
                }]
                return summary, "document_summary", sources
            except Exception as e:
                return f"‚ùå Impossible de g√©n√©rer le r√©sum√© : {str(e)}", "error", None

    # === Cas demande explicative (RAG avec FAISS) ===
    question_embedding = encodeur(new_message)
    sims = cosine_similarity([question_embedding], embeddings)[0]
    top_k = np.argsort(sims)[-10:][::-1]
    articles_selectionnes = [textes[i] for i in top_k]

    # Extraire les noms de documents sources pour les m√©tadonn√©es
    sources = []
    for i in top_k:
        texte = textes[i]
        # Extraire le nom du document (premi√®re partie avant "article")
        doc_name = texte.split(" article ")[0] if " article " in texte else "Document juridique"
        if doc_name not in [s.get("document") for s in sources]:
            sources.append({"document": doc_name, "relevance": float(sims[i])})

    # Limiter √† 5 sources principales
    sources = sources[:5]

    contexte = "\n\n".join(articles_selectionnes)

    # Construire l'historique de messages pour Mistral
    mistral_messages = [
        {
            "role": "system",
            "content": "Tu es un assistant juridique sp√©cialis√© en droit burkinab√® (Burkina Faso). R√©ponds de mani√®re pr√©cise en citant les articles et les lois utilis√©s."
        }
    ]

    # Ajouter l'historique de la conversation (seulement les messages user/assistant, pas les r√©f√©rences)
    for item in history:
        if isinstance(item, dict) and "role" in item:
            mistral_messages.append({
                "role": item["role"],
                "content": item["content"]
            })

    # Ajouter le nouveau message avec le contexte
    mistral_messages.append({
        "role": "user",
        "content": f"Contexte juridique :\n{contexte}\n\nQuestion : {new_message}"
    })

    response = generate_mistral_complete(mistral_messages)
    return response, "legal_answer", sources


# ==========================================================
# üåê Routes Flask
# ==========================================================
@app.route("/")
def index():
    return render_template("index.html")


@app.route("/api/chat", methods=["POST", "OPTIONS"])
def api_chat():
    """
    Endpoint principal pour le frontend Angular.

    Requ√™te:
        {
            "conversationId": "conv-1729459200-k8j3h2l9q",
            "message": "Quelle est la proc√©dure..."
        }

    R√©ponse:
        {
            "id": "msg-1729459201-xyz789",
            "conversationId": "conv-1729459200-k8j3h2l9q",
            "content": "Pour cr√©er une entreprise...",
            "role": "assistant",
            "timestamp": "2025-10-22T14:30:01.000Z"
        }
    """
    # G√©rer les requ√™tes OPTIONS pour CORS
    if request.method == "OPTIONS":
        return "", 200

    try:
        # 1. Extraire et valider les donn√©es
        data = request.get_json()

        if not data:
            return jsonify({"error": "Request body must be JSON"}), 400

        conversation_id = data.get("conversationId", "").strip()
        message = data.get("message", "").strip()

        # Validation
        if not conversation_id:
            return jsonify({"error": "conversationId is required"}), 400

        if not message:
            return jsonify({"error": "message is required"}), 400

        if len(message) > 5000:
            return jsonify({"error": "message is too long (max 5000 characters)"}), 400

        # 2. Sauvegarder le message utilisateur dans l'historique
        conversations_history[conversation_id].append({
            "role": "user",
            "content": message
        })

        # 3. Traiter la question avec contexte et r√©cup√©rer les m√©tadonn√©es
        ai_response, response_type, sources = process_question_with_context(conversation_id, message)

        # 4. Valider et nettoyer la r√©ponse (convertir relevance en %, remplacer [] par None)
        ai_response, sources = validate_response(ai_response, response_type, sources)

        # 5. G√©n√©rer un ID pour le message assistant
        assistant_message_id = generate_message_id()

        # 6. Sauvegarder la r√©ponse dans l'historique
        conversations_history[conversation_id].append({
            "role": "assistant",
            "content": ai_response
        })

        # 7. Retourner la r√©ponse structur√©e au format attendu par le frontend
        response_data = {
            "id": assistant_message_id,
            "conversationId": conversation_id,
            "content": ai_response,
            "role": "assistant",
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "metadata": {
                "responseType": response_type,
                "country": "Burkina Faso",
                "sources": sources
            }
        }

        return jsonify(response_data), 200

    except Exception as e:
        # Log l'erreur pour le debugging
        print(f"‚ùå Error in /api/chat: {str(e)}")
        import traceback
        traceback.print_exc()

        return jsonify({
            "error": "An error occurred processing your request",
            "details": str(e) if app.debug else None
        }), 500


@app.route("/stream", methods=["POST"])
def stream():
    data = request.get_json()
    question = data.get("question", "").strip().lower()

    # === Cas r√©sum√© ===
    if question in ["oui", "oui merci", "r√©sume", "r√©sume-moi", "je veux un r√©sum√©"]:
        ref = session.get("derniere_reference")
        if ref:
            nom_fichier = ref["lien"].split("/")[-1]
            chemin_pdf = f"./static/pdfs/{nom_fichier}"

            try:
                texte_complet = extract_text_from_pdf(chemin_pdf)
                prompt = (
                    f"Voici le contenu d'un texte juridique :\n\n{texte_complet}\n\n"
                    "Fais un r√©sum√© clair et structur√© de ce document."
                )

                def event_stream():
                    for chunk in generate_mistral_stream(prompt):
                        yield chunk
                    yield "[DONE]"

                return Response(event_stream(), mimetype="text/plain")

            except Exception as e:
                def event_stream():
                    yield f"‚ùå Impossible de g√©n√©rer le r√©sum√© : {str(e)}"

                return Response(event_stream(), mimetype="text/plain")

    # === Cas recherche de document ===
    type_question = detecte_type_question(question)
    if type_question == "recherche":
        type_texte, numero = extraire_reference_loi_decret(question)
        lien_pdf = rechercher_dans_metadatas(type_texte, numero, metadatas)

        if lien_pdf:
            session["derniere_reference"] = {
                "type": type_texte,
                "numero": numero,
                "lien": lien_pdf,
            }
            message = (
                f"üìÑ Voici le document demand√© : "
                f"<a href='{lien_pdf}' target='_blank'>cliquer ici</a><br>"
                f"Souhaitez-vous un r√©sum√© ? (oui/non)"
            )

            def event_stream():
                yield message

            return Response(event_stream(), mimetype="text/plain")
        else:
            def event_stream():
                yield "‚ùå R√©f√©rence non trouv√©e dans les m√©tadonn√©es."

            return Response(event_stream(), mimetype="text/plain")

    # === Cas demande explicative (RAG avec FAISS) ===
    question_embedding = encodeur(question)
    sims = cosine_similarity([question_embedding], embeddings)[0]
    top_k = np.argsort(sims)[-10:][::-1]
    articles_selectionnes = [textes[i] for i in top_k]

    contexte = "\n\n".join(articles_selectionnes)
    prompt = (
        f"Contexte :\n{contexte}\n\n"
        f"Question : {question}\n\n"
        "R√©ponds de mani√®re pr√©cise en citant les articles et les lois utilis√©s."
    )

    def event_stream():
        for chunk in generate_mistral_stream(prompt):
            yield chunk
        yield "[DONE]"

    return Response(event_stream(), mimetype="text/plain")


# ==========================================================
# ‚ñ∂Ô∏è Lancement de l‚Äôapplication
# ==========================================================
if __name__ == "__main__":
    app.run(debug=True, threaded=True)
