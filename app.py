from flask import Flask, render_template, request, Response, session, jsonify
from flask_cors import CORS
import time
import re
import pickle
import unicodedata
import numpy as np
import pandas as pd
import os
import cv2
import pytesseract
from PIL import Image
from PyPDF2 import PdfReader
from pdf2image import convert_from_path
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
from mistralai import Mistral
from datetime import datetime
from collections import defaultdict


# ==========================================================
# üöÄ Configuration Flask
# ==========================================================
app = Flask(__name__)
app.secret_key = "resume_secret_key"

# Configuration CORS pour le frontend Angular
CORS(app, origins=["http://localhost:4200"], supports_credentials=True)

# Stockage en m√©moire des conversations (remplacer par DB en production)
conversations_history = defaultdict(list)


# ==========================================================
# ‚öôÔ∏è Configuration Tesseract OCR (Windows)
# ==========================================================
pytesseract.pytesseract.tesseract_cmd = (
    r"C:\\Users\\DELL 7540\\AppData\\Local\\Programs\\Tesseract-OCR\\tesseract.exe"
)
os.environ["TESSDATA_PREFIX"] = (
    r"C:\\Users\\DELL 7540\\AppData\\Local\\Programs\\Tesseract-OCR\\tessdata"
)


# ==========================================================
# üì¶ Chargement des mod√®les et donn√©es
# ==========================================================
encoder_model = SentenceTransformer(
    "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)
client = Mistral(api_key="uhSmZH1rHb7TPZdmoSnjVRGMrDPtJDe6")
mistral_model = "mistral-small-latest"

with open("faiss_index/metadatas.pkl", "rb") as f:
    metadatas = pickle.load(f)

embeddings = np.load("faiss_index/embeddings.npy")
df = pd.read_csv("faiss_index/fichier.csv")
textes = df["texte"].tolist()


# ==========================================================
# üõ†Ô∏è Fonctions utilitaires
# ==========================================================
def preprocess_image(pil_img):
    img = np.array(pil_img.convert("L"))
    img = cv2.GaussianBlur(img, (5, 5), 0)
    _, img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    return Image.fromarray(img)


def extract_text_ocr(pdf_path):
    images = convert_from_path(pdf_path, dpi=300)
    full_text = ""
    for img in images:
        preproc = preprocess_image(img)
        text = pytesseract.image_to_string(preproc, lang="fra", config="--psm 6")
        full_text += "\n" + text
    return full_text


def extract_text_from_pdf(path):
    try:
        reader = PdfReader(path)
        text = "\n".join(page.extract_text() or "" for page in reader.pages)
        if len(text.strip()) < 100:  # si texte trop court ‚Üí fallback OCR
            return extract_text_ocr(path)
        return text
    except Exception:
        return extract_text_ocr(path)


def detecte_type_question(question):
    motifs = (
        r"\b(cherche|cherches|chercher|recherche|recherches|donne|donner|donne[-\s]?moi|"
        r"fournis|fournir|trouve|trouver|obtiens|obtenir|je veux|j'aimerais voir|"
        r"montre[-\s]?moi|acc√®de|acc√©der √†|affiche|afficher)\b"
    )
    return "recherche" if re.search(motifs, question.lower()) else "demande"


def extraire_reference_loi_decret(question):
    question = question.lower()
    pattern = r"(loi|d√©cret)[\s\-n¬∞]*([n]?\s?\d{1,4}(?:[\-/]\d{1,4})?[\-\_/]?\d{1,4}?)"
    match = re.search(pattern, question)
    if match:
        type_texte = match.group(1)
        numero = match.group(2).replace(" ", "")
        return type_texte.capitalize(), numero
    return None, None


def enlever_accents(texte):
    return "".join(
        c for c in unicodedata.normalize("NFD", texte) if unicodedata.category(c) != "Mn"
    )


def normaliser_numero(numero):
    parties = re.split(r"[-/]", numero)
    if len(parties) == 2:
        a, b = str(int(parties[0])), str(int(parties[1]))
        return [f"{a}-{b}", f"{b}-{a}"]
    return [numero]


def rechercher_dans_metadatas(type_texte, numero, metadatas):
    if type_texte and numero:
        variantes = normaliser_numero(numero)
        prefix = enlever_accents(type_texte.upper())
        for metadata in metadatas:
            loi_metadata = enlever_accents(str(metadata["loi"]).upper())
            for variante in variantes:
                terme = f"{prefix}_{variante}"
                if terme in loi_metadata:
                    return metadata["lien_pdf"]
    return None


def encodeur(question):
    return encoder_model.encode(question)


def generate_mistral_stream(prompt):
    response = client.chat.complete(
        model=mistral_model, messages=[{"role": "user", "content": prompt}]
    )
    full_text = response.choices[0].message.content

    # Nettoyage du texte
    cleaned_text = re.sub(r"#+\s*", "", full_text)

    # Streaming par paquets de 50 caract√®res
    for i in range(0, len(cleaned_text), 50):
        yield cleaned_text[i : i + 50]
        time.sleep(0.1)


def generate_mistral_complete(messages):
    """
    G√©n√®re une r√©ponse compl√®te (non-streaming) √† partir d'un historique de messages.

    Args:
        messages: Liste de messages au format [{"role": "user|assistant", "content": "..."}]

    Returns:
        str: R√©ponse compl√®te de Mistral
    """
    response = client.chat.complete(
        model=mistral_model,
        messages=messages
    )
    full_text = response.choices[0].message.content

    # Nettoyage du texte
    cleaned_text = re.sub(r"#+\s*", "", full_text)
    return cleaned_text


def generate_message_id():
    """G√©n√®re un ID unique pour un message."""
    import random
    timestamp = int(datetime.now().timestamp() * 1000)
    random_part = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz0123456789', k=9))
    return f"msg-{timestamp}-{random_part}"


def process_question_with_context(conversation_id, new_message):
    """
    Traite une question en utilisant le contexte de la conversation.

    Args:
        conversation_id: ID de la conversation
        new_message: Nouveau message utilisateur

    Returns:
        tuple: (r√©ponse de l'assistant, type de r√©ponse, sources utilis√©es)
    """
    # R√©cup√©rer l'historique de la conversation
    history = conversations_history[conversation_id]

    # D√©tecter le type de question
    type_question = detecte_type_question(new_message)

    # === Cas recherche de document ===
    if type_question == "recherche":
        type_texte, numero = extraire_reference_loi_decret(new_message)
        lien_pdf = rechercher_dans_metadatas(type_texte, numero, metadatas)

        if lien_pdf:
            response = (
                f"üìÑ Voici le document demand√© : "
                f'<a href="{lien_pdf}" target="_blank">cliquer ici</a><br>'
                f"Souhaitez-vous un r√©sum√© ? (oui/non)"
            )
            # Stocker la r√©f√©rence dans l'historique pour usage ult√©rieur
            conversations_history[conversation_id].append({
                "type": "reference",
                "lien": lien_pdf,
                "type_texte": type_texte,
                "numero": numero
            })
            sources = [{"type": type_texte, "numero": numero, "lien": lien_pdf}]
            return response, "document_link", sources
        else:
            return "‚ùå R√©f√©rence non trouv√©e dans les m√©tadonn√©es.", "not_found", []

    # === Cas r√©sum√© (si l'utilisateur r√©pond oui apr√®s une recherche de doc) ===
    if new_message.lower() in ["oui", "oui merci", "r√©sume", "r√©sume-moi", "je veux un r√©sum√©"]:
        # Chercher la derni√®re r√©f√©rence dans l'historique
        last_reference = None
        for item in reversed(history):
            if isinstance(item, dict) and item.get("type") == "reference":
                last_reference = item
                break

        if last_reference:
            nom_fichier = last_reference["lien"].split("/")[-1]
            chemin_pdf = f"./static/pdfs/{nom_fichier}"

            try:
                texte_complet = extract_text_from_pdf(chemin_pdf)
                prompt_messages = [
                    {
                        "role": "system",
                        "content": "Tu es un assistant juridique sp√©cialis√© dans les textes juridiques du Burkina Faso. Tu dois faire des r√©sum√©s clairs, structur√©s et factuels."
                    },
                    {
                        "role": "user",
                        "content": (
                            f"Voici le contenu d'un texte juridique burkinab√® :\n\n{texte_complet}\n\n"
                            "Fais un r√©sum√© structur√© de ce document en pr√©sentant :\n"
                            "1. L'objet du texte\n"
                            "2. Les points cl√©s\n"
                            "3. Les dispositions importantes"
                        )
                    }
                ]
                summary = generate_mistral_complete(prompt_messages)
                sources = [{
                    "type": last_reference.get("type_texte"),
                    "numero": last_reference.get("numero"),
                    "lien": last_reference["lien"]
                }]
                return summary, "document_summary", sources
            except Exception as e:
                return f"‚ùå Impossible de g√©n√©rer le r√©sum√© : {str(e)}", "error", []

    # === Cas demande explicative (RAG avec FAISS) ===
    question_embedding = encodeur(new_message)
    sims = cosine_similarity([question_embedding], embeddings)[0]
    top_k = np.argsort(sims)[-10:][::-1]
    articles_selectionnes = [textes[i] for i in top_k]

    # Extraire les noms de documents sources pour les m√©tadonn√©es
    sources = []
    for i in top_k:
        texte = textes[i]
        # Extraire le nom du document (premi√®re partie avant "article")
        doc_name = texte.split(" article ")[0] if " article " in texte else "Document juridique"
        if doc_name not in [s.get("document") for s in sources]:
            sources.append({"document": doc_name, "relevance": float(sims[i])})

    # Limiter √† 5 sources principales
    sources = sources[:5]

    contexte = "\n\n".join(articles_selectionnes)

    # Construire l'historique de messages pour Mistral
    mistral_messages = [
        {
            "role": "system",
            "content": """Tu es un assistant juridique sp√©cialis√© en droit burkinab√® (Burkina Faso).

R√àGLES STRICTES :
1. Utilise UNIQUEMENT les informations du contexte juridique fourni ci-dessous
2. Ne JAMAIS inventer ou supposer des informations qui ne sont pas dans le contexte
3. Si l'information demand√©e n'est pas dans le contexte, r√©ponds : "Je n'ai pas trouv√© cette information dans les textes juridiques disponibles du Burkina Faso"
4. Cite PR√âCIS√âMENT les articles, lois, d√©crets et arr√™t√©s du Burkina Faso en utilisant leur num√©ro exact
5. Structure tes r√©ponses clairement avec des paragraphes et des sections

FORMAT DE R√âPONSE :
- Commence par une r√©ponse directe √† la question
- Cite les r√©f√©rences l√©gales exactes (ex: "Selon l'article 5 de l'arr√™t√© n¬∞016/2023")
- Utilise des paragraphes distincts pour chaque point
- Sois pr√©cis, factuel et professionnel"""
        }
    ]

    # Ajouter l'historique de la conversation (seulement les messages user/assistant, pas les r√©f√©rences)
    for item in history:
        if isinstance(item, dict) and "role" in item:
            mistral_messages.append({
                "role": item["role"],
                "content": item["content"]
            })

    # Ajouter le nouveau message avec le contexte
    mistral_messages.append({
        "role": "user",
        "content": f"CONTEXTE JURIDIQUE (Burkina Faso) :\n{contexte}\n\nQUESTION : {new_message}\n\nRappel : Utilise UNIQUEMENT les informations du contexte ci-dessus."
    })

    response = generate_mistral_complete(mistral_messages)
    return response, "legal_answer", sources


# ==========================================================
# üåê Routes Flask
# ==========================================================
@app.route("/")
def index():
    return render_template("index.html")


@app.route("/api/chat", methods=["POST", "OPTIONS"])
def api_chat():
    """
    Endpoint principal pour le frontend Angular.

    Requ√™te:
        {
            "conversationId": "conv-1729459200-k8j3h2l9q",
            "message": "Quelle est la proc√©dure..."
        }

    R√©ponse:
        {
            "id": "msg-1729459201-xyz789",
            "conversationId": "conv-1729459200-k8j3h2l9q",
            "content": "Pour cr√©er une entreprise...",
            "role": "assistant",
            "timestamp": "2025-10-22T14:30:01.000Z"
        }
    """
    # G√©rer les requ√™tes OPTIONS pour CORS
    if request.method == "OPTIONS":
        return "", 200

    try:
        # 1. Extraire et valider les donn√©es
        data = request.get_json()

        if not data:
            return jsonify({"error": "Request body must be JSON"}), 400

        conversation_id = data.get("conversationId", "").strip()
        message = data.get("message", "").strip()

        # Validation
        if not conversation_id:
            return jsonify({"error": "conversationId is required"}), 400

        if not message:
            return jsonify({"error": "message is required"}), 400

        if len(message) > 5000:
            return jsonify({"error": "message is too long (max 5000 characters)"}), 400

        # 2. Sauvegarder le message utilisateur dans l'historique
        conversations_history[conversation_id].append({
            "role": "user",
            "content": message
        })

        # 3. Traiter la question avec contexte et r√©cup√©rer les m√©tadonn√©es
        ai_response, response_type, sources = process_question_with_context(conversation_id, message)

        # 4. G√©n√©rer un ID pour le message assistant
        assistant_message_id = generate_message_id()

        # 5. Sauvegarder la r√©ponse dans l'historique
        conversations_history[conversation_id].append({
            "role": "assistant",
            "content": ai_response
        })

        # 6. Retourner la r√©ponse structur√©e au format attendu par le frontend
        response_data = {
            "id": assistant_message_id,
            "conversationId": conversation_id,
            "content": ai_response,
            "role": "assistant",
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "metadata": {
                "responseType": response_type,
                "country": "Burkina Faso",
                "sources": sources
            }
        }

        return jsonify(response_data), 200

    except Exception as e:
        # Log l'erreur pour le debugging
        print(f"‚ùå Error in /api/chat: {str(e)}")
        import traceback
        traceback.print_exc()

        return jsonify({
            "error": "An error occurred processing your request",
            "details": str(e) if app.debug else None
        }), 500


@app.route("/stream", methods=["POST"])
def stream():
    data = request.get_json()
    question = data.get("question", "").strip().lower()

    # === Cas r√©sum√© ===
    if question in ["oui", "oui merci", "r√©sume", "r√©sume-moi", "je veux un r√©sum√©"]:
        ref = session.get("derniere_reference")
        if ref:
            nom_fichier = ref["lien"].split("/")[-1]
            chemin_pdf = f"./static/pdfs/{nom_fichier}"

            try:
                texte_complet = extract_text_from_pdf(chemin_pdf)
                prompt = (
                    f"Voici le contenu d'un texte juridique :\n\n{texte_complet}\n\n"
                    "Fais un r√©sum√© clair et structur√© de ce document."
                )

                def event_stream():
                    for chunk in generate_mistral_stream(prompt):
                        yield chunk
                    yield "[DONE]"

                return Response(event_stream(), mimetype="text/plain")

            except Exception as e:
                def event_stream():
                    yield f"‚ùå Impossible de g√©n√©rer le r√©sum√© : {str(e)}"

                return Response(event_stream(), mimetype="text/plain")

    # === Cas recherche de document ===
    type_question = detecte_type_question(question)
    if type_question == "recherche":
        type_texte, numero = extraire_reference_loi_decret(question)
        lien_pdf = rechercher_dans_metadatas(type_texte, numero, metadatas)

        if lien_pdf:
            session["derniere_reference"] = {
                "type": type_texte,
                "numero": numero,
                "lien": lien_pdf,
            }
            message = (
                f"üìÑ Voici le document demand√© : "
                f"<a href='{lien_pdf}' target='_blank'>cliquer ici</a><br>"
                f"Souhaitez-vous un r√©sum√© ? (oui/non)"
            )

            def event_stream():
                yield message

            return Response(event_stream(), mimetype="text/plain")
        else:
            def event_stream():
                yield "‚ùå R√©f√©rence non trouv√©e dans les m√©tadonn√©es."

            return Response(event_stream(), mimetype="text/plain")

    # === Cas demande explicative (RAG avec FAISS) ===
    question_embedding = encodeur(question)
    sims = cosine_similarity([question_embedding], embeddings)[0]
    top_k = np.argsort(sims)[-10:][::-1]
    articles_selectionnes = [textes[i] for i in top_k]

    contexte = "\n\n".join(articles_selectionnes)
    prompt = (
        f"Contexte :\n{contexte}\n\n"
        f"Question : {question}\n\n"
        "R√©ponds de mani√®re pr√©cise en citant les articles et les lois utilis√©s."
    )

    def event_stream():
        for chunk in generate_mistral_stream(prompt):
            yield chunk
        yield "[DONE]"

    return Response(event_stream(), mimetype="text/plain")


# ==========================================================
# ‚ñ∂Ô∏è Lancement de l‚Äôapplication
# ==========================================================
if __name__ == "__main__":
    app.run(debug=True, threaded=True)
